{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5181d591cea5ff1",
   "metadata": {},
   "source": [
    "The first thing we do is to import the necessary libraries. As seen in the dataset github repository (https://github.com/RomainLITUD/conflict_resolution_dataset) we need to import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41051d7e72b2ed1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:26:17.895145Z",
     "start_time": "2025-04-16T11:26:16.443133Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import zarr\n",
    "import os\n",
    "from dataset.visual_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3e385ab90a894",
   "metadata": {},
   "source": [
    "Read and visualize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56a797544bb651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:26:20.774358Z",
     "start_time": "2025-04-16T11:26:20.549797Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_av = 'av'\n",
    "folder_hv = 'hv'\n",
    "\n",
    "root_av = './dataset/data_3m/'+folder_av+'/'\n",
    "root_hv = './dataset/data_3m/'+folder_hv+'/'\n",
    "\n",
    "log_ids_av = [name for name in os.listdir(root_av) if os.path.isdir(os.path.join(root_av, name))]\n",
    "log_ids_hv = [name for name in os.listdir(root_hv) if os.path.isdir(os.path.join(root_hv, name))]\n",
    "\n",
    "print('Number of scenarios for Autonomous Vehicles: ', len(log_ids_av))\n",
    "print('Number of scenarios for Human-driven Vehicles: ', len(log_ids_hv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b019d2fb21715d",
   "metadata": {},
   "source": [
    "Read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3582b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:26:25.470632Z",
     "start_time": "2025-04-16T11:26:25.460987Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "slices: len = nb_objects + 1, slices[n] and slices[n+1] gives the start/end indices of the n-th object\n",
    "maps: lanes as NumPy array\n",
    "type: len = nb_objects, contains 7 numbers with the following meanings:\n",
    "        -1: Static background\n",
    "        0: human-driven vehicles\n",
    "        1: pedestrians\n",
    "        2: motorcyclists\n",
    "        3: cyclists\n",
    "        4: buses\n",
    "        10: autonomous vehicles\n",
    "timestep: timestamps in second, timestep[slices[n]: slices[n+1]] give the timestamps for the n-th object\n",
    "motion: motion state, with 7 dimensions\n",
    "    motion[slices[n]: slices[n+1]] gives the motion of the n-th object, the 7 features are the following variables in order:\n",
    "        [x, y, vx, vy, ax, ay, yaw]\n",
    "        yaw is to the x-axis, between [-pi, pi]\n",
    "'''\n",
    "slices, timestep, motion, type, maps = read_scenario(log_ids_av[2441], root_av)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f2f03",
   "metadata": {},
   "source": [
    "Visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58908621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:26:28.554660Z",
     "start_time": "2025-04-16T11:26:28.431994Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = visualize(log_ids_av[25], root_av, other_road_users=True, direction=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc12ddf",
   "metadata": {},
   "source": [
    "Filter the data:\n",
    "\n",
    "The metafile contains the following information:\n",
    "log_id: string, index of the scenario\n",
    "[xi_start, yi_start]: float, direction vector of the first* agent recorded in the scenario at the start** time\n",
    "[xj_start, yj_start]: float, direction vector of the second agent recorded in the scenario at the start time\n",
    "typei: str, agent type of the first agent recorded in the scenario, being one of {'AV','HV','Pedestrian','Motorcyclist','Cyclist','Bus'}\n",
    "[xi_end, yi_end]: float, direction vector of the first* agent recorded in the scenario at the end*** time\n",
    "[xj_end, yj_end]: float, direction vector of the second agent recorded in the scenario at the end time\n",
    "typej: str, agent type of the second agent recorded in the scenario, being one of {'AV','HV','Pedestrian','Motorcyclist','Cyclist','Bus'}\n",
    "direction: str, whether the second-passing vehicle moved from the left ('L-R') or the right ('R-L') of the first-passing agent\n",
    "PET: float, post-encroachment-time\n",
    "ifirst: bool, whether the first-passing agent is the first agent recorded in the scenario\n",
    "angle_start: float, angle between the direction vectors of the two agents at the start time\n",
    "angle_end: float, angle between the direction vectors of the two agents at the end time\n",
    "start: str, whether the two agents ran parallel (P), crossed (C), or ran opposite (O) to each other before reaching the conflict point\n",
    "end: str, whether the two agents ran parallel (P), crossed (C), or ran opposite (O) to each other after reaching the conflict point\n",
    "\n",
    "Notes:\n",
    "    * Note that the first agent does not necessarily pass the conflict point first.\n",
    "    ** We consider the start time as 5 seconds before the first-passing agent passed the conflict point, or the start of the record if the time before passing the conflict point is less than 5 seconds.\n",
    "    *** Similarly, the end time is 5 seconds after the second-passing vehicle passed the conflict point, or the end of the record if the time after passing the conflict point is less than 5 seconds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147ab84",
   "metadata": {},
   "source": [
    "We'll load the metafile and filter for intersection scenarios based on the 'angle_start' and 'angle_end' fields, which indicate crossing trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c20a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:27:01.470030Z",
     "start_time": "2025-04-16T11:27:01.421700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load metafile for autonomous vehicles\n",
    "metafile_av = pd.read_csv('./dataset/metafile_av.csv')\n",
    "\n",
    "# Filter for intersection scenarios (crossing trajectories)\n",
    "intersection_cases = metafile_av[\n",
    "    ((metafile_av['start'] == 'cross') | (metafile_av['end'] == 'cross')) &\n",
    "    (metafile_av['typej'] != 'Pedestrian')\n",
    "]\n",
    "\n",
    "print(f\"Total number of intersection scenarios: {len(intersection_cases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06353478",
   "metadata": {},
   "source": [
    "We define three functions, one to get the scenario filename, one to analyze the scenario and one to visualize the scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bac9cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:27:03.369168Z",
     "start_time": "2025-04-16T11:27:03.360174Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_scenario_filename(scenario_id, root_path):\n",
    "    \"\"\"\n",
    "    Maps a scenario ID to its corresponding zarr file in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        scenario_id (str): The ID of the scenario to find\n",
    "        root_path (str): Root directory containing the scenario files\n",
    "        \n",
    "    Returns:\n",
    "        str or None: Filename if found, None if no matching file exists\n",
    "    \"\"\"\n",
    "    zarr_files = [f for f in os.listdir(root_path) if f.endswith('.zarr')]\n",
    "    \n",
    "    # First try exact format matching\n",
    "    target_suffix = f\"{int(scenario_id):02d}.zarr\"\n",
    "    matching_files = [f for f in zarr_files if f.endswith(target_suffix)]\n",
    "    \n",
    "    if not matching_files:\n",
    "        # If no exact match, search for any file containing the scenario number\n",
    "        matching_files = [f for f in zarr_files if str(int(scenario_id)) in f]\n",
    "    \n",
    "    if not matching_files:\n",
    "        print(f\"Error: No matching file found for scenario {scenario_id}\")\n",
    "        return None\n",
    "        \n",
    "    return matching_files[0]\n",
    "\n",
    "def analyze_intersection_scenario(scenario_id, root_path):\n",
    "    \"\"\"\n",
    "    Analyzes a single intersection scenario and extracts relevant features for conflict detection.\n",
    "    \n",
    "    Args:\n",
    "        scenario_id (str): The ID of the scenario to analyze\n",
    "        root_path (str): Root directory containing the scenario files\n",
    "        \n",
    "    Returns:\n",
    "        dict or None: Dictionary containing scenario features if successful:\n",
    "            - scenario_id: ID of the analyzed scenario\n",
    "            - trajectory_length: Number of timesteps in the scenario\n",
    "            - vehicle_count: Number of vehicles in the scenario\n",
    "            - motion_data: Vehicle motion states (position, velocity, acceleration, yaw)\n",
    "            - timestep: Timestamps for each motion state\n",
    "            - slices: Indices marking different vehicles' data\n",
    "            - type_data: Vehicle type information\n",
    "        Returns None if analysis fails\n",
    "    \"\"\"\n",
    "    filename = get_scenario_filename(scenario_id, root_path)\n",
    "    if filename is None:\n",
    "        print(f\"Error: No matching file found for scenario {scenario_id}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        slices, timestep, motion, type_data, maps = read_scenario(filename, root_path)\n",
    "        \n",
    "        features = {\n",
    "            'scenario_id': scenario_id,\n",
    "            'trajectory_length': len(timestep),\n",
    "            'vehicle_count': len(type_data),\n",
    "            'motion_data': motion,\n",
    "            'timestep': timestep,\n",
    "            'slices': slices,\n",
    "            'type_data': type_data\n",
    "        }\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing scenario {scenario_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def visualize_scenario(scenario_id, root_path):\n",
    "    \"\"\"Visualizes a single scenario with trajectory information.\"\"\"\n",
    "    try:\n",
    "        filename = get_scenario_filename(scenario_id, root_path)\n",
    "        if filename:\n",
    "            fig, ax = visualize(filename, root_path, \n",
    "                              other_road_users=True, \n",
    "                              direction=True)\n",
    "            plt.title(f'Scenario {scenario_id}')\n",
    "            return fig, ax\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing scenario {scenario_id}: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebeed55",
   "metadata": {},
   "source": [
    "Now, we can analyze a few sample scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98fd290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:27:10.662693Z",
     "start_time": "2025-04-16T11:27:09.959458Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 5\n",
    "sample_scenarios = intersection_cases['log_id'].iloc[:sample_size]\n",
    "\n",
    "print(\"\\nAnalyzing sample scenarios:\")\n",
    "for scenario_id in sample_scenarios:\n",
    "    print(f\"\\nProcessing scenario {scenario_id}\")\n",
    "    features = analyze_intersection_scenario(scenario_id, root_av)\n",
    "    \n",
    "    if features:\n",
    "        print(f\"Successfully analyzed scenario:\")\n",
    "        print(f\"- Trajectory length: {features['trajectory_length']}\")\n",
    "        print(f\"- Number of vehicles: {features['vehicle_count']}\")\n",
    "    else:\n",
    "        print(\"Analysis failed\")\n",
    "\n",
    "# Visualize sample scenarios\n",
    "fig, axs = plt.subplots(1, min(5, len(sample_scenarios)), figsize=(20, 4))\n",
    "if not isinstance(axs, np.ndarray):\n",
    "    axs = [axs]\n",
    "\n",
    "for i, scenario_id in enumerate(sample_scenarios[:5]):\n",
    "    _, _ = visualize_scenario(scenario_id, root_av)\n",
    "    if i < len(axs):\n",
    "        axs[i].set_title(f'Scenario {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210a84b",
   "metadata": {},
   "source": [
    "Now we can do the conflict analysis for the intersection scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca5d2c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:27:18.139703Z",
     "start_time": "2025-04-16T11:27:18.117854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants for conflict analysis\n",
    "CONFLICT_THRESHOLDS = {\n",
    "    'TTC_CRITICAL': 2.0,     # Critical Time-to-Collision (seconds)\n",
    "    'PET_CRITICAL': 1.0,     # Critical Post-Encroachment Time (seconds)\n",
    "    'ANGLE_THRESHOLD': {\n",
    "        'CROSSING': 45,      # Minimum angle for crossing conflict (degrees)\n",
    "        'HEAD_ON': 150       # Minimum angle for head-on conflict (degrees)\n",
    "    },\n",
    "    'DISTANCE_CRITICAL': 5.0 # Critical distance (meters)\n",
    "}\n",
    "\n",
    "class ConflictType:\n",
    "    \"\"\"Possible conflict types in autonomous driving scenarios\"\"\"\n",
    "    CROSSING = \"crossing\"           # Trajectories intersect at an angle\n",
    "    REAR_END = \"rear_end\"          # Following vehicle conflicts with leading vehicle\n",
    "    HEAD_ON = \"head_on\"            # Vehicles approaching from opposite directions\n",
    "    MERGING = \"merging\"            # Vehicle merging into traffic\n",
    "    NO_CONFLICT = \"no_conflict\"    # No conflict detected\n",
    "\n",
    "def calculate_time_to_collision(ego_motion, other_motion):\n",
    "    \"\"\"\n",
    "    Calculates Time-to-Collision (TTC) between two vehicles\n",
    "    \n",
    "    Args:\n",
    "        ego_motion: Motion data for ego vehicle [x, y, vx, vy, ...]\n",
    "        other_motion: Motion data for other vehicle [x, y, vx, vy, ...]\n",
    "    \n",
    "    Returns:\n",
    "        float: Minimum TTC value or infinity if no collision course\n",
    "    \"\"\"\n",
    "    # Interpolate trajectories to common length\n",
    "    target_length = 100\n",
    "    \n",
    "    # Create normalized time arrays for interpolation\n",
    "    t_ego = np.linspace(0, 1, len(ego_motion))\n",
    "    t_other = np.linspace(0, 1, len(other_motion))\n",
    "    t_common = np.linspace(0, 1, target_length)\n",
    "    \n",
    "    # Initialize interpolated arrays\n",
    "    ego_interp = np.zeros((target_length, ego_motion.shape[1]))\n",
    "    other_interp = np.zeros((target_length, other_motion.shape[1]))\n",
    "    \n",
    "    # Interpolate each component\n",
    "    for i in range(ego_motion.shape[1]):\n",
    "        ego_interp[:, i] = np.interp(t_common, t_ego, ego_motion[:, i])\n",
    "        other_interp[:, i] = np.interp(t_common, t_other, other_motion[:, i])\n",
    "    \n",
    "    # Extract positions and velocities from interpolated data\n",
    "    ego_pos = ego_interp[:, :2]    # [x, y]\n",
    "    ego_vel = ego_interp[:, 2:4]   # [vx, vy]\n",
    "    other_pos = other_interp[:, :2]\n",
    "    other_vel = other_interp[:, 2:4]\n",
    "    \n",
    "    # Calculate relative velocity and distance\n",
    "    rel_pos = ego_pos - other_pos\n",
    "    rel_vel = ego_vel - other_vel\n",
    "    \n",
    "    # Calculate TTC\n",
    "    distance = np.linalg.norm(rel_pos, axis=1)\n",
    "    rel_speed = np.linalg.norm(rel_vel, axis=1)\n",
    "    \n",
    "    # Avoid division by zero and negative relative speeds\n",
    "    valid_idx = (rel_speed > 0.1)\n",
    "    if not np.any(valid_idx):\n",
    "        return float('inf')\n",
    "    \n",
    "    ttc = distance[valid_idx] / rel_speed[valid_idx]\n",
    "    return np.min(ttc) if len(ttc) > 0 else float('inf')\n",
    "\n",
    "def calculate_post_encroachment_time(ego_motion, other_motion, conflict_point=None):\n",
    "    \"\"\"\n",
    "    Calculates Post-Encroachment Time (PET) at the conflict point\n",
    "    \n",
    "    Args:\n",
    "        ego_motion: Motion data for ego vehicle\n",
    "        other_motion: Motion data for other vehicle\n",
    "        conflict_point: Optional pre-defined conflict point\n",
    "    \n",
    "    Returns:\n",
    "        float: PET value in seconds\n",
    "    \"\"\"\n",
    "    # Interpolate trajectories to common length\n",
    "    target_length = 100\n",
    "    \n",
    "    # Create normalized time arrays for interpolation\n",
    "    t_ego = np.linspace(0, 1, len(ego_motion))\n",
    "    t_other = np.linspace(0, 1, len(other_motion))\n",
    "    t_common = np.linspace(0, 1, target_length)\n",
    "    \n",
    "    # Initialize interpolated arrays\n",
    "    ego_interp = np.zeros((target_length, ego_motion.shape[1]))\n",
    "    other_interp = np.zeros((target_length, other_motion.shape[1]))\n",
    "    \n",
    "    # Interpolate each component\n",
    "    for i in range(ego_motion.shape[1]):\n",
    "        ego_interp[:, i] = np.interp(t_common, t_ego, ego_motion[:, i])\n",
    "        other_interp[:, i] = np.interp(t_common, t_other, other_motion[:, i])\n",
    "    \n",
    "    if conflict_point is None:\n",
    "        # Estimate conflict point as the closest point between trajectories\n",
    "        ego_pos = ego_interp[:, :2]\n",
    "        other_pos = other_interp[:, :2]\n",
    "        distances = np.linalg.norm(ego_pos[:, np.newaxis] - other_pos, axis=2)\n",
    "        min_idx = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "        conflict_point = (ego_pos[min_idx[0]] + other_pos[min_idx[1]]) / 2\n",
    "    \n",
    "    # Calculate arrival times at conflict point using interpolated data\n",
    "    ego_times = calculate_arrival_time(ego_interp, conflict_point)\n",
    "    other_times = calculate_arrival_time(other_interp, conflict_point)\n",
    "    \n",
    "    # PET is the absolute difference between arrival times\n",
    "    return abs(ego_times - other_times)\n",
    "\n",
    "def calculate_arrival_time(motion, point):\n",
    "    \"\"\"\n",
    "    Calculates time of arrival to a specific point\n",
    "    \n",
    "    Args:\n",
    "        motion: Vehicle motion data\n",
    "        point: Target point coordinates\n",
    "    \n",
    "    Returns:\n",
    "        float: Estimated arrival time in seconds\n",
    "    \"\"\"\n",
    "    positions = motion[:, :2]\n",
    "    velocities = motion[:, 2:4]\n",
    "    \n",
    "    # Find closest point to conflict point\n",
    "    distances = np.linalg.norm(positions - point, axis=1)\n",
    "    closest_idx = np.argmin(distances)\n",
    "    \n",
    "    # Calculate time based on distance and speed\n",
    "    speed = np.linalg.norm(velocities[closest_idx])\n",
    "    if speed < 0.1:  # Almost stopped\n",
    "        return float('inf')\n",
    "    \n",
    "    return distances[closest_idx] / speed\n",
    "\n",
    "def classify_conflict_type(ego_motion, other_motion):\n",
    "    \"\"\"\n",
    "    Classifies the type of conflict based on vehicle trajectories\n",
    "    \n",
    "    Args:\n",
    "        ego_motion: Motion data for ego vehicle\n",
    "        other_motion: Motion data for other vehicle\n",
    "    \n",
    "    Returns:\n",
    "        str: Type of conflict (CROSSING, REAR_END, HEAD_ON, MERGING)\n",
    "    \"\"\"\n",
    "    # Calculate angle between trajectories\n",
    "    ego_direction = ego_motion[-1, 2:4] - ego_motion[0, 2:4]\n",
    "    other_direction = other_motion[-1, 2:4] - other_motion[0, 2:4]\n",
    "    \n",
    "    angle = np.arccos(np.dot(ego_direction, other_direction) / \n",
    "                     (np.linalg.norm(ego_direction) * np.linalg.norm(other_direction)))\n",
    "    angle_deg = np.degrees(angle)\n",
    "    \n",
    "    # Classify based on angle\n",
    "    if angle_deg > CONFLICT_THRESHOLDS['ANGLE_THRESHOLD']['HEAD_ON']:\n",
    "        return ConflictType.HEAD_ON\n",
    "    elif angle_deg > CONFLICT_THRESHOLDS['ANGLE_THRESHOLD']['CROSSING']:\n",
    "        return ConflictType.CROSSING\n",
    "    else:\n",
    "        # Determine if rear-end or merging\n",
    "        relative_position = other_motion[0, :2] - ego_motion[0, :2]\n",
    "        heading_difference = abs(ego_motion[0, 6] - other_motion[0, 6])\n",
    "        \n",
    "        if heading_difference < np.pi/4:  # Similar directions\n",
    "            return ConflictType.REAR_END\n",
    "        else:\n",
    "            return ConflictType.MERGING\n",
    "\n",
    "def assess_risk_level(ttc, pet, distance):\n",
    "    \"\"\"\n",
    "    Evaluates risk level based on multiple metrics\n",
    "    \n",
    "    Args:\n",
    "        ttc: Time-to-Collision value\n",
    "        pet: Post-Encroachment Time value\n",
    "        distance: Minimum distance between vehicles\n",
    "    \n",
    "    Returns:\n",
    "        str: Risk level (HIGH, MEDIUM, LOW)\n",
    "    \"\"\"\n",
    "    if ttc < CONFLICT_THRESHOLDS['TTC_CRITICAL'] or \\\n",
    "       pet < CONFLICT_THRESHOLDS['PET_CRITICAL'] or \\\n",
    "       distance < CONFLICT_THRESHOLDS['DISTANCE_CRITICAL']:\n",
    "        return \"HIGH\"\n",
    "    elif ttc < CONFLICT_THRESHOLDS['TTC_CRITICAL'] * 2 or \\\n",
    "         pet < CONFLICT_THRESHOLDS['PET_CRITICAL'] * 2 or \\\n",
    "         distance < CONFLICT_THRESHOLDS['DISTANCE_CRITICAL'] * 2:\n",
    "        return \"MEDIUM\"\n",
    "    else:\n",
    "        return \"LOW\"\n",
    "\n",
    "def analyze_scenario_conflicts(scenario_data):\n",
    "    \"\"\"\n",
    "    Complete conflict analysis for a scenario\n",
    "    \n",
    "    Args:\n",
    "        scenario_data: Dictionary containing scenario information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results including conflict type, metrics, and risk level\n",
    "    \"\"\"\n",
    "    # Extract motion data for ego and other vehicles\n",
    "    ego_motion = scenario_data['motion_data'][scenario_data['slices'][0]:scenario_data['slices'][1]]\n",
    "    other_motion = scenario_data['motion_data'][scenario_data['slices'][1]:scenario_data['slices'][2]]\n",
    "    \n",
    "    # Interpolate trajectories to common length for minimum distance calculation\n",
    "    target_length = 100\n",
    "    t_ego = np.linspace(0, 1, len(ego_motion))\n",
    "    t_other = np.linspace(0, 1, len(other_motion))\n",
    "    t_common = np.linspace(0, 1, target_length)\n",
    "    \n",
    "    ego_interp = np.zeros((target_length, ego_motion.shape[1]))\n",
    "    other_interp = np.zeros((target_length, other_motion.shape[1]))\n",
    "    \n",
    "    for i in range(ego_motion.shape[1]):\n",
    "        ego_interp[:, i] = np.interp(t_common, t_ego, ego_motion[:, i])\n",
    "        other_interp[:, i] = np.interp(t_common, t_other, other_motion[:, i])\n",
    "    \n",
    "    # Calculate main metrics using interpolated data\n",
    "    ttc = calculate_time_to_collision(ego_motion, other_motion)\n",
    "    pet = calculate_post_encroachment_time(ego_motion, other_motion)\n",
    "    min_distance = np.min(np.linalg.norm(ego_interp[:, :2] - other_interp[:, :2], axis=1))\n",
    "    \n",
    "    # Classify conflict type (using interpolated data)\n",
    "    conflict_type = classify_conflict_type(ego_interp, other_interp)\n",
    "    \n",
    "    # Evaluate risk level\n",
    "    risk_level = assess_risk_level(ttc, pet, min_distance)\n",
    "    \n",
    "    return {\n",
    "        'scenario_id': scenario_data['scenario_id'],\n",
    "        'conflict_type': conflict_type,\n",
    "        'metrics': {\n",
    "            'TTC': ttc,\n",
    "            'PET': pet,\n",
    "            'min_distance': min_distance,\n",
    "            'risk_level': risk_level\n",
    "        },\n",
    "        'timestamp': scenario_data['timestep']\n",
    "    }\n",
    "\n",
    "def analyze_all_scenarios(intersection_cases, root_path):\n",
    "    \"\"\"\n",
    "    Analyzes all scenarios and generates a report\n",
    "    \n",
    "    Args:\n",
    "        intersection_cases: DataFrame containing scenario metadata\n",
    "        root_path: Root directory containing scenario files\n",
    "    \n",
    "    Returns:\n",
    "        list: Analysis results for all scenarios\n",
    "    \"\"\"\n",
    "    conflict_analyses = []\n",
    "    \n",
    "    for _, case in intersection_cases.iterrows():\n",
    "        scenario_id = case['log_id']\n",
    "        features = analyze_intersection_scenario(scenario_id, root_path)\n",
    "        \n",
    "        if features:\n",
    "            conflict_analysis = analyze_scenario_conflicts(features)\n",
    "            conflict_analyses.append(conflict_analysis)\n",
    "            \n",
    "            print(f\"\\nAnalysis for Scenario {scenario_id}:\")\n",
    "            print(f\"Conflict Type: {conflict_analysis['conflict_type']}\")\n",
    "            print(f\"Risk Level: {conflict_analysis['metrics']['risk_level']}\")\n",
    "            print(f\"TTC: {conflict_analysis['metrics']['TTC']:.2f} seconds\")\n",
    "            print(f\"PET: {conflict_analysis['metrics']['PET']:.2f} seconds\")\n",
    "            print(f\"Minimum Distance: {conflict_analysis['metrics']['min_distance']:.2f} meters\")\n",
    "    \n",
    "    return conflict_analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f437e",
   "metadata": {},
   "source": [
    "Run the analysis for 10 scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06e43e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T11:27:25.813148Z",
     "start_time": "2025-04-16T11:27:25.739893Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nAnalyzing conflicts in scenarios...\")\n",
    "conflict_analyses = analyze_all_scenarios(intersection_cases[:10], root_av)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a62300",
   "metadata": {},
   "source": [
    "Now we can prepare the data to train the DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e111cfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:06:55.386049Z",
     "start_time": "2025-04-17T19:06:55.369374Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dqn_environment(scenario_data):\n",
    "    \"\"\"\n",
    "    Prepares scenario data for DQN training by creating a reinforcement learning environment.\n",
    "    \n",
    "    Args:\n",
    "        scenario_data (dict): Dictionary containing scenario features from analyze_intersection_scenario\n",
    "        \n",
    "    Returns:\n",
    "        IntersectionEnv: An environment wrapper for DQN training with:\n",
    "            - State space: 7-dimensional vector containing relative distances, velocities, and angles\n",
    "            - Action space: 4 discrete actions (STOP, SLOW_DOWN, MAINTAIN, ACCELERATE)\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_state_features(motion_data, slices, timestep):\n",
    "        \"\"\"\n",
    "        Extracts relevant features for the state representation from vehicle trajectories.\n",
    "        \n",
    "        Args:\n",
    "            motion_data: Raw motion data for all vehicles\n",
    "            slices: Indices marking different vehicles' data\n",
    "            timestep: Timestamps for motion data\n",
    "            \n",
    "        Returns:\n",
    "            numpy.array: 7-dimensional state vector containing:\n",
    "                - Minimum distance between vehicles\n",
    "                - Average relative velocity\n",
    "                - Minimum time to intersection\n",
    "                - Current yaw angle of ego vehicle\n",
    "                - Current yaw angle of other vehicle\n",
    "                - Current speed of ego vehicle\n",
    "                - Current speed of other vehicle\n",
    "        \"\"\"\n",
    "        # Get ego vehicle (AV) and other vehicle trajectories\n",
    "        ego_motion = motion_data[slices[0]:slices[1]]\n",
    "        other_motion = motion_data[slices[1]:slices[2]]\n",
    "        \n",
    "        # Interpolate trajectories to common length\n",
    "        target_length = 100  # número fijo de puntos\n",
    "        \n",
    "        # Crear array de tiempo normalizado para interpolación\n",
    "        t_ego = np.linspace(0, 1, len(ego_motion))\n",
    "        t_other = np.linspace(0, 1, len(other_motion))\n",
    "        t_common = np.linspace(0, 1, target_length)\n",
    "        \n",
    "        # Interpolar cada componente\n",
    "        ego_interp = np.zeros((target_length, ego_motion.shape[1]))\n",
    "        other_interp = np.zeros((target_length, other_motion.shape[1]))\n",
    "        \n",
    "        for i in range(ego_motion.shape[1]):\n",
    "            ego_interp[:, i] = np.interp(t_common, t_ego, ego_motion[:, i])\n",
    "            other_interp[:, i] = np.interp(t_common, t_other, other_motion[:, i])\n",
    "        \n",
    "        # Calcular características con las trayectorias interpoladas\n",
    "        relative_distance = np.linalg.norm(ego_interp[:, :2] - other_interp[:, :2], axis=1)\n",
    "        relative_velocity = np.linalg.norm(ego_interp[:, 2:4] - other_interp[:, 2:4], axis=1)\n",
    "        time_to_intersection = relative_distance / (relative_velocity + 1e-6)  # Avoid division by zero\n",
    "        \n",
    "        # Combine features into state vector\n",
    "        state = np.array([\n",
    "            relative_distance.min(),          # Minimum distance between vehicles\n",
    "            relative_velocity.mean(),         # Average relative velocity\n",
    "            time_to_intersection.min(),       # Minimum time to intersection\n",
    "            ego_interp[-1, 6],               # Current yaw angle of ego vehicle\n",
    "            other_interp[-1, 6],             # Current yaw angle of other vehicle\n",
    "            ego_interp[-1, 2:4].mean(),      # Current speed of ego vehicle\n",
    "            other_interp[-1, 2:4].mean()     # Current speed of other vehicle\n",
    "        ])\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def calculate_reward(state, action, next_state):\n",
    "        \"\"\"\n",
    "        Calculates the reward for a given state-action transition.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state vector\n",
    "            action: Action taken (0-3)\n",
    "            next_state: Resulting state vector\n",
    "            \n",
    "        Returns:\n",
    "            float: Reward value based on:\n",
    "                - Safety (distance between vehicles)\n",
    "                - Efficiency (maintaining appropriate speed)\n",
    "                - Collision penalties\n",
    "        \"\"\"\n",
    "        # Extract metrics from states\n",
    "        current_distance = state[0]\n",
    "        next_distance = next_state[0]\n",
    "        current_velocity = state[5]\n",
    "        # Apply safe distance penalty with log-scale\n",
    "        if next_distance > IntersectionEnv.MIN_SAFE_DISTANCE:\n",
    "            safety_reward = IntersectionEnv.SAFE_REWARD\n",
    "        else:\n",
    "            # Penalize for unsafe distance\n",
    "            # Use log scale to avoid extreme penalties\n",
    "            safety_reward = -IntersectionEnv.COLLISION_PENALTY * np.log1p(1 / (next_distance + 1e-6))\n",
    "\n",
    "        # Encourage smooth, efficient driving\n",
    "        efficiency_reward = (\n",
    "        IntersectionEnv.EFFICIENCY_FACTOR * current_velocity\n",
    "        if action != 0 else -1)  # discourage unnecessary stops\n",
    "\n",
    "        # Combine rewards\n",
    "        reward = safety_reward + efficiency_reward\n",
    "\n",
    "        # Clip to prevent extreme spikes\n",
    "        reward = np.clip(reward, -200, 50)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    class IntersectionEnv:\n",
    "        \"\"\"\n",
    "        Environment wrapper for DQN training that simulates intersection scenarios.\n",
    "        \n",
    "        Attributes:\n",
    "            COLLISION_PENALTY (float): Penalty for unsafe distances/collisions\n",
    "            SAFE_REWARD (float): Reward for maintaining safe distance\n",
    "            EFFICIENCY_FACTOR (float): Weight for speed-based rewards\n",
    "            MIN_SAFE_DISTANCE (float): Minimum safe distance between vehicles\n",
    "        \"\"\"\n",
    "        # Constantes de la clase\n",
    "        COLLISION_PENALTY = -100\n",
    "        SAFE_REWARD = 10\n",
    "        EFFICIENCY_FACTOR = 0.5\n",
    "        MIN_SAFE_DISTANCE = 5.0  # meters\n",
    "        \n",
    "        def __init__(self, scenario_data):\n",
    "            self.scenario_data = scenario_data\n",
    "            self.current_step = 0\n",
    "            self.max_steps = len(scenario_data['timestep'])\n",
    "            self.state = None\n",
    "            \n",
    "        def reset(self):\n",
    "            self.current_step = 0\n",
    "            self.state = extract_state_features(\n",
    "                self.scenario_data['motion_data'],\n",
    "                self.scenario_data['slices'],\n",
    "                self.scenario_data['timestep']\n",
    "            )\n",
    "            return self.state\n",
    "        \n",
    "        def step(self, action):\n",
    "            # Execute action and get next state\n",
    "            self.current_step += 1\n",
    "            next_state = extract_state_features(\n",
    "                self.scenario_data['motion_data'],\n",
    "                self.scenario_data['slices'],\n",
    "                self.scenario_data['timestep']\n",
    "            )\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = calculate_reward(self.state, action, next_state)\n",
    "            \n",
    "            # Check if episode is done\n",
    "            done = self.current_step >= self.max_steps or reward <= -self.COLLISION_PENALTY\n",
    "            \n",
    "            self.state = next_state\n",
    "            return next_state, reward, done, {}\n",
    "\n",
    "    return IntersectionEnv(scenario_data)\n",
    "\n",
    "def prepare_training_data(intersection_cases, root_path):\n",
    "    \"\"\"\n",
    "    Prepares a dataset of intersection scenarios for DQN training.\n",
    "    \n",
    "    Args:\n",
    "        intersection_cases (pandas.DataFrame): DataFrame containing intersection scenario metadata\n",
    "        root_path (str): Root directory containing scenario files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of IntersectionEnv objects ready for training\n",
    "    \"\"\"\n",
    "    training_environments = []\n",
    "    \n",
    "    for _, case in intersection_cases.iterrows():\n",
    "        scenario_id = case['log_id']\n",
    "        features = analyze_intersection_scenario(scenario_id, root_path)\n",
    "        \n",
    "        if features:\n",
    "            env = prepare_dqn_environment(features)\n",
    "            training_environments.append(env)\n",
    "    \n",
    "    return training_environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9f4c3",
   "metadata": {},
   "source": [
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f230363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:06:59.326648Z",
     "start_time": "2025-04-17T19:06:59.246216Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Preparing training environments...\")\n",
    "training_envs = prepare_training_data(intersection_cases[:10], root_av)  # Start with 10 scenarios for testing\n",
    "print(f\"Prepared {len(training_envs)} training environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd66428e27a5b57e",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "The logistic regression model is a simple linear model that predicts the probability of a conflict based on the extracted features. The model is trained using the features obtained from the scenarios and the corresponding labels indicating whether a conflict occurred or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb88a9e6d4b5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c9d3bcc87de163",
   "metadata": {},
   "source": [
    "### Define the DQN Agent and Training Loop\n",
    "\n",
    "The DQN agent is a simple feedforward neural network that takes the **state** as input and outputs **Q-values** for each possible action. The training loop implements the DQN algorithm, which includes **experience replay** and **target network updates** for stability.\n",
    "\n",
    "The DQN agent is trained using experiences collected from the environment. The training loop includes the following steps:\n",
    "\n",
    "1. **Initialize** the DQN agent and the target network.\n",
    "2. **Initialize** the replay buffer.\n",
    "3. For each episode:\n",
    "    - **1** Reset the environment and get the initial state.\n",
    "    - **2** Use the ε-greedy policy to select an action (either explore or choose the action with the highest Q-value).\n",
    "    - **3** Interact with the environment to collect an experience tuple (next state, reward, done flag).\n",
    "    - **4** Store the experience in the replay buffer.\n",
    "    - **5** Sample a random batch of experiences from the replay buffer.\n",
    "    - **6** Compute the current Q-values and the target Q-values using the **Bellman equation**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5f88805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:07:02.911007Z",
     "start_time": "2025-04-17T19:07:02.903616Z"
    }
   },
   "outputs": [],
   "source": [
    "#Implement the DQN agent and training loop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\"\"\"\n",
    "    DQNAgent class implements a simple feedforward neural network for the DQN agent.\n",
    "    It takes the state dimension and action dimension as input and initializes the network.\n",
    "    The forward method defines the forward pass through the network.\n",
    "    The act method implements an epsilon-greedy policy for action selection.\n",
    "    The agent uses Adam optimizer and Mean Squared Error loss function for training.\n",
    "\"\"\"\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_dim=7, action_dim=4, lr=1e-3):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, 3)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(torch.FloatTensor(state))\n",
    "        return int(torch.argmax(q_values))\n",
    "\"\"\"\n",
    "    ReplayBuffer class stores past experiences so the agent can learn from them.\n",
    "    It uses a deque to maintain a fixed size buffer and allows sampling of random batches for training.\n",
    "    Stores transitions as: (state, action, reward, next_state, done)\n",
    "\"\"\"\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.FloatTensor(states),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(next_states),\n",
    "            torch.tensor(dones, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3dfddbb35a6aa6",
   "metadata": {},
   "source": [
    "### Train the DQN Agent\n",
    "\n",
    "The `train_dqn` function performs the training of the DQN agent using the environment. It implements the DQN algorithm with support for experience replay, epsilon-greedy action selection, and periodic target network updates.\n",
    "\n",
    "The training process consists of the following steps:\n",
    "\n",
    "1. **Initialize** the policy network (DQN agent) and the target network with the same weights.\n",
    "2. **Set up** the optimizer (e.g., Adam) for updating the policy network.\n",
    "3. **Create** a replay buffer to store experiences collected from the environment.\n",
    "4. For each episode:\n",
    "    - **4.1** Reset the environment and obtain the initial state.\n",
    "    - **4.2** For each time step in the episode:\n",
    "        - **4.2.1** Select an action using the ε-greedy strategy:\n",
    "            - With probability ε, choose a random action (**exploration**).\n",
    "            - Otherwise, choose the action with the highest Q-value predicted by the policy network (**exploitation**).\n",
    "        - **4.2.2** Apply the action in the environment to receive the next state, reward, and done flag.\n",
    "        - **4.2.3** Store the experience `(state, action, reward, next_state, done)` in the replay buffer.\n",
    "        - **4.2.4** If the buffer has enough samples:\n",
    "            - Sample a random batch of experiences.\n",
    "            - Compute target Q-values using the **Bellman equation**:\n",
    "\n",
    "              Q_traget = r + γ * max_a * Q_target(s', a) * (1-done)\n",
    "\n",
    "            - Compute current Q-values predicted by the policy network.\n",
    "            - Compute the **loss** between the current Q-values and the target Q-values (typically MSE loss).\n",
    "            - Perform a **gradient descent step** to update the policy network.\n",
    "    - **4.3** Periodically update the target network by copying the weights from the policy network.\n",
    "    - **4.4** Decay ε over time to gradually shift from exploration to exploitation.\n",
    "\n",
    "This loop continues until the desired number of episodes is completed, gradually improving the agent's ability to make optimal decisions based on its experience.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9503326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:07:06.628894Z",
     "start_time": "2025-04-17T19:07:06.620772Z"
    }
   },
   "outputs": [],
   "source": [
    "#DQN Training Loop\n",
    "\"\"\"\n",
    "    train_dqn function implements the training loop for the DQN agent.\n",
    "    It initializes the agent, target network, and replay buffer.\n",
    "    The function runs for a specified number of episodes, collecting experiences and training the agent.\n",
    "    It uses epsilon-greedy exploration strategy and updates the target network periodically.\n",
    "\"\"\"\n",
    "def train_dqn(envs, num_episodes=1000, batch_size=64, gamma=0.99,\n",
    "              epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.995,\n",
    "              target_update_freq=10):\n",
    "\n",
    "    state_dim = 7\n",
    "    action_dim = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize main and target networks\n",
    "    policy_net = DQNAgent(state_dim, action_dim).to(device)\n",
    "    target_net = DQNAgent(state_dim, action_dim).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    buffer = ReplayBuffer()\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    rewards_history = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env = random.choice(envs)  # Random scenario environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy_net.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if reward > 1000:\n",
    "                print(f\"[Warning] Episode {episode}, BIG reward detected: {reward}\")\n",
    "\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "                # Move to device\n",
    "                states = states.to(device)\n",
    "                actions = actions.to(device)\n",
    "                rewards = rewards.to(device)\n",
    "                next_states = next_states.to(device)\n",
    "\n",
    "                if not isinstance(dones, torch.Tensor):\n",
    "                    dones = torch.tensor(dones, dtype=torch.float32)\n",
    "                else:\n",
    "                    dones = dones.clone().detach().float()\n",
    "                dones = dones.to(device)\n",
    "\n",
    "                # Q(s,a)\n",
    "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # max_a' Q_target(s', a')\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states).max(1)[0]\n",
    "\n",
    "                target_q = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = policy_net.loss_fn(q_values, target_q)\n",
    "\n",
    "                policy_net.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                policy_net.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        # Update target network\n",
    "        if episode % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    return policy_net, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ecbb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:07:16.040605Z",
     "start_time": "2025-04-17T19:07:15.615030Z"
    }
   },
   "outputs": [],
   "source": [
    "trained_agent, reward_curve = train_dqn(training_envs, num_episodes=500)\n",
    "\n",
    "def smooth_curve(curve, window=10):\n",
    "    return np.convolve(curve, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot\n",
    "smoothed = smooth_curve(reward_curve, window=10)\n",
    "plt.plot(smoothed)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Smoothed Total Reward')\n",
    "plt.title('Training Progress (Smoothed)')\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conflict-resolution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
